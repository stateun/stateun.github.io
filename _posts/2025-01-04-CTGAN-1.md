---
layout: single
title: "CTGAN Code Review"
date: 2025-01-03
author_profile: true
use_math: true
categories:
  - Math
tags:
  - Lipschitz Condition
  - Gumbel Softmax
  - Varational Gaussian Mixture Model
permalink: /CTGAN-1/
---

## Lipschitz Condition

\[
\textbf{Definition (Lipschitz Condition).} \\
\text{함수 } f: \mathcal{X} \rightarrow \mathbb{R}\text{ 가 주어졌을 때, 모든 }x, y \in \mathcal{X}\text{에 대하여}\\
\quad |f(x) - f(y)| \le K \cdot \|x - y\| \\
\text{를 만족하면, } f \text{는 } K\text{-Lipschitz 함수라고 한다.}
\]

- \( K \in \mathbb{R}_{\ge 0} \)는 리프시츠 상수(Lipschitz constant)입니다.
- \(\|x-y\|\)는 \(x\)와 \(y\)의 거리(예: 유클리드 거리)를 나타냅니다.

---

## 2. Gumbel Softmax

\[
\textbf{Definition 2 (Gumbel-Softmax).}\\
\text{카테고리 분포 } \mathrm{Cat}\bigl(\pi_1, \ldots, \pi_K\bigr) \text{에서 표본을 뽑고자 하자.}\\
\text{각 클래스 } i \in \{1, \ldots, K\} \text{에 대해} \\
g_i = -\log\bigl(-\log(u_i)\bigr), \quad u_i \sim \mathrm{Uniform}(0,1) \\
\text{로 정의되는 독립 Gumbel 잡음 } \{g_i\}_{i=1}^K \text{을 샘플링한다.}\\
\text{그 다음, 온도 매개변수 } \tau > 0 \text{를 사용하여}\\
y_i = \frac{\exp\Bigl(\bigl(\log \pi_i + g_i\bigr)/\tau\Bigr)}
{\sum_{j=1}^K \exp\Bigl(\bigl(\log \pi_j + g_j\bigr)/\tau\Bigr)}
\]
\[
\text{로 정의한다. 이 때 } y = (y_1, \dots, y_K) \text{는}\\
\text{원-핫 벡터와 유사한 '부드러운' 표본 벡터로 볼 수 있다.}
\]


---

## 3. Variational Gaussian Mixture Model (V-GMM)

\[
\textbf{Definition 3 (Variational Gaussian Mixture Model).}\\
\text{데이터 집합 } X = \{x_1, \ldots, x_N\} \subset \mathbb{R}^D \text{가 주어져 있다고 하자.}\\
\text{가우시안 혼합 모델(Gaussian Mixture Model, GMM)은 }K\text{개의}\\
\text{혼합 계수(mixing coefficient) } \{\pi_k\}_{k=1}^K,\;
\text{평균 } \{\mu_k\}_{k=1}^K,\;
\text{공분산 } \{\Sigma_k\}_{k=1}^K \\
\text{으로 구성되며, 각 데이터 } x_n \text{에 대해}\\
p(x_n \mid \pi, \mu, \Sigma) 
= \sum_{k=1}^K \pi_k \,\mathcal{N}(x_n \mid \mu_k, \Sigma_k)
\]
\[
\text{로 정의된다. 여기서 } \sum_{k=1}^K \pi_k = 1,\;\; \pi_k \ge 0.
\]
\[
\text{베이즈 관점에서는 파라미터 } (\pi, \{\mu_k\}, \{\Sigma_k\}) \text{에}\\
\text{사전(prior) 분포를 부여하고,}\\
\text{잠재변수 } z_n \in \{1,\ldots,K\} \text{가 각 } x_n \text{에 대해}\\
p(z_n = k) = \pi_k, \quad p(x_n \mid z_n = k) = \mathcal{N}(x_n \mid \mu_k, \Sigma_k)
\]
\text{를 만족한다고 본다.}
\]
\[
\text{그러나 사후분포 } p(\pi, \mu, \Sigma, z \mid X) \text{를 직접 계산하기 어려우므로,}\\
\text{변분추론(Variational Inference)을 통해 근사한다.}
\]
\[
\text{구체적으로, } q(\pi, \mu, \Sigma, z) \text{를 도입하여}\\
q(\pi, \mu, \Sigma, z) \approx p(\pi, \mu, \Sigma, z \mid X),
\]
\[
\text{Evidence Lower Bound (ELBO) } \mathcal{L}(q) \text{를 최대화하는 방식으로 } q \text{를 학습한다:}\\
\mathcal{L}(q) 
= \mathbb{E}_{q}\bigl[\log p(X, \pi, \mu, \Sigma, z)\bigr]
- \mathbb{E}_{q}\bigl[\log q(\pi, \mu, \Sigma, z)\bigr].
\]
\[
\text{이 과정을 통해 } q \text{가 실제 사후분포에 가까워지도록 유도한다.}
\]

