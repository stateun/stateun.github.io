---
layout: single
title: "[MLDL2] Homework 4"
date: 2025-01-03
categories:
  - Deep Learning
tags:
  - Knowledge Distillation
author: "Sung Eun Lee"
toc: true
---

# Knowledge Distillation on CIFAR-100

- **Deadline** ðŸ“…: December 15, 2024, 11:59 PM  
- **Score** ðŸŒŸ: 30/30  

This homework investigates **Knowledge Distillation** for the **CIFAR-100 dataset**, where a pretrained teacher model (ResNet-50) is used to guide the training of a smaller student model. By comparing **validation accuracies**, 
we demonstrate the effectiveness of distillation under computational constraints.

## Summary
- **Keywords**: Knowledge Distillation, ResNet, CIFAR-100, Teacher-Student  
- **Performance**:  
  - ResNet-8 Student: ~35.0% validation accuracy  
  - ResNet-14 Student: ~38.5% validation accuracy  
- **Limitations**:  
  - Overfitting due to limited data diversity  
  - Loss plots were not sufficiently discriminative  
  - Computational overhead for teacher model inference

## GitHub Repository
- Below, you can find the code for this assignment:  
  [![GitHub](https://img.shields.io/badge/GitHub-Repository-black?logo=github)](https://github.com/stateun/MLDL2/tree/main/Transfer_leraning)
