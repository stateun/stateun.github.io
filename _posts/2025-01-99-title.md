---
layout: single
title: "[MLDL2] Homework 4"
date: 2025-01-03
author_profile: true
categories:
  - Deep Learning
tags:
  - Knowledge Distillation
  - Ensemble Learning
toc: true
---

# Knowledge Distillation and Ensemble Strategies for CIFAR-100 Classification

- **Submission Date** ðŸ“…: December 20, 2024  
- **Score** ðŸŒŸ: 30/30  

This homework investigates **knowledge distillation and ensemble learning** techniques using the **CIFAR-100 dataset**. Various ResNet models were trained without pre-trained weights to analyze the effect of knowledge distillation and ensemble strategies.

## Summary
- **Keywords**: Knowledge Distillation, ResNet, Ensemble Learning, CIFAR-100
- **Performance**:
  - ResNet-14 (Single Model): ~49% validation accuracy
  - ResNet-14 Ensemble: ~59.8% validation accuracy
- **Limitations**:
  - CUDA out-of-memory issues for deeper models (e.g., ResNet-34)
  - Limited improvement despite hyperparameter optimization

## Methods
### Hyperparameter Settings
- **Learning Rate**: `0.1` (Optimized for stability and convergence speed)
- **Momentum**: `0.9` (Accelerates gradients for faster convergence)
- **Weight Decay**: `1e-4` (Prevents overfitting)
- **Batch Size**: `128` (Balanced memory and stability)
- **Epochs**: `50` (Sufficient for convergence)
- **Temperature Scaling**: `4` (Enhanced teacher-to-student transfer)
- **Label Smoothing**: `0.1` (Improved generalization)
- **Ensemble Size**: `10` (Reduced to `8` for ResNet-18 due to memory constraints)

### Knowledge Distillation
- **Loss Functions**:
  - CrossEntropy Loss with Label Smoothing: Reduced overconfidence.
  - KL Divergence: Softened logits for knowledge transfer.
- **Dynamic Adjustment**:
  - Balancing weights between supervised loss and distillation loss (`lambda = 0.5`).

### Ensemble Learning
- Multiple student models were trained independently.
- Ensemble predictions were formed by averaging softmax probabilities.
- Validation and test predictions were saved for evaluation.

## Results
- **Single Model Performance**:
  - ResNet-14 achieved the best single-model accuracy (~49% validation accuracy).
- **Ensemble Performance**:
  - ResNet-14 Ensemble achieved ~59.8% validation accuracy.
  - **Memory Constraints**:
    - ResNet-18 and ResNet-34 faced CUDA out-of-memory issues.

## GitHub Repository
- Below, you can find the code for this assignment:  
  [![GitHub](https://img.shields.io/badge/GitHub-Repository-black?logo=github)](https://github.com/stateun/MLDL2/tree/main/Transfer_learning)
