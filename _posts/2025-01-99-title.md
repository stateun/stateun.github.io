---
layout: single
title: "[MLDL2] Homework 4"
date: 2025-01-03
author_profile: true
mathjax: true
categories:
  - Deep Learning
tags:
  - Knowledge Distillation
---

# Knowledge Distillation and Ensemble Strategies for CIFAR-100 Classification

- **Submission Date** ðŸ“…: December 20, 2024  
- **Score** ðŸŒŸ: 30/30  

This homework investigates **knowledge distillation and ensemble learning** techniques using the **CIFAR-100 dataset**. Various ResNet models were trained without pre-trained weights to analyze the effect of knowledge distillation and ensemble strategies.

## Summary
- **Keywords**: Knowledge Distillation, ResNet, Ensemble Learning, CIFAR-100
- **Performance**:
  - ResNet-14 (Single Model): ~49% validation accuracy
  - ResNet-14 Ensemble: ~59.8% validation accuracy
- **Limitations**:
  - CUDA out-of-memory issues for deeper models (e.g., ResNet-34)
  - Limited improvement despite hyperparameter optimization

## Methods
### Hyperparameter Settings
- **Learning Rate** (\( \eta \)): `0.1`  
  \( \eta \) controls the step size during gradient descent. A value of 0.1 was chosen to balance convergence speed and stability.
- **Momentum** (\( \mu \)): `0.9`  
  Momentum (\( \mu \)) helps accelerate gradient vectors in the right direction, leading to faster convergence. The value \( \mu = 0.9 \) is standard for SGD.
- **Weight Decay** (\( \lambda \)): `1e-4`  
  A regularization term \( \lambda \|w\|^2 \) was used to prevent overfitting without hindering convergence.
- **Batch Size** (\( B \)): `128`  
  The number of samples per gradient update (\( B = 128 \)) balanced memory efficiency and gradient stability.
- **Epochs** (\( E \)): `50`  
  The total number of training iterations (\( E = 50 \)) was sufficient for convergence without overfitting.
- **Temperature Scaling** (\( T \)): `4`  
  Used to soften logits from the teacher model. Higher temperatures (\( T = 4 \)) encourage better knowledge transfer by reducing sharp probabilities.
- **Label Smoothing** (\( \alpha \)): `0.1`  
  Regularizes the classification task by modifying true label distributions. For a true label \( y \), it smooths the distribution as:  
  \[
  y_{\text{smooth}} = (1 - \alpha) y + \frac{\alpha}{C},
  \]
  where \( C \) is the number of classes.
- **Ensemble Size** (\( N \)): `10`  
  The number of student models in the ensemble (\( N = 10 \)), reduced to \( N = 8 \) for ResNet-18 due to memory limitations.


### Knowledge Distillation
- **Loss Functions**:
  - CrossEntropy Loss with Label Smoothing: Reduced overconfidence.
  - KL Divergence: Softened logits for knowledge transfer.
- **Dynamic Adjustment**:
  - Balancing weights between supervised loss and distillation loss (`lambda = 0.5`).

### Ensemble Learning
- Multiple student models were trained independently.
- Ensemble predictions were formed by averaging softmax probabilities.
- Validation and test predictions were saved for evaluation.

## Results
- **Single Model Performance**:
  - ResNet-14 achieved the best single-model accuracy (~49% validation accuracy).
- **Ensemble Performance**:
  - ResNet-14 Ensemble achieved ~59.8% validation accuracy.
  - **Memory Constraints**:
    - ResNet-18 and ResNet-34 faced CUDA out-of-memory issues.

## GitHub Repository
- Below, you can find the code for this assignment:  
  [![GitHub](https://img.shields.io/badge/GitHub-Repository-black?logo=github)](https://github.com/stateun/MLDL2/tree/main/Transfer_learning)
